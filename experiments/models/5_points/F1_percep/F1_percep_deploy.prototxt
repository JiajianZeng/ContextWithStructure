name: "F1_percep_224x224_rgb"

layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape: {dim: 1 dim: 3 dim: 224 dim: 224}
  }
}


layer {
    name: "lm_conv1"
    type: "Convolution"
    bottom: "data"
    top: "lm_conv1"
    convolution_param {
        num_output: 20
        kernel_size: 4
    }
}

#layer {
#  bottom: "lm_conv1"
#  top: "lm_conv1"
#  name: "bn_lm_conv1"
#  type: "BatchNorm"
  #batch_norm_param {
  #  use_global_stats: false
  #}
#}

#layer {
#  bottom: "lm_conv1"
#  top: "lm_conv1"
#  name: "scale_lm_conv1"
#  type: "Scale"
#  scale_param {
#    bias_term: true
#  }
#}

layer {
    name: "lm_relu1"
    type: "ReLU"
    bottom: "lm_conv1"
    top: "lm_conv1"
}

layer {
    name: "lm_pool1"
    type: "Pooling"
    bottom: "lm_conv1"
    top: "lm_pool1"
    pooling_param {
        pool: MAX
        kernel_size: 2
        stride: 2
    }
}

layer {
    name: "lm_conv2"
    type: "Convolution"
    bottom: "lm_pool1"
    top: "lm_conv2"
    convolution_param {
        num_output: 40
        kernel_size: 3
    }
}

#layer {
#  bottom: "lm_conv2"
#  top: "lm_conv2"
#  name: "bn_lm_conv2"
#  type: "BatchNorm"
  #batch_norm_param {
  #  use_global_stats: false
  #}
#}

#layer {
#  bottom: "lm_conv2"
#  top: "lm_conv2"
#  name: "scale_lm_conv2"
#  type: "Scale"
#  scale_param {
#    bias_term: true
#  }
#}

layer {
    name: "lm_relu2"
    type: "ReLU"
    bottom: "lm_conv2"
    top: "lm_conv2"
}

layer {
    name: "lm_pool2"
    type: "Pooling"
    bottom: "lm_conv2"
    top: "lm_pool2"
    pooling_param {
        pool: MAX
        kernel_size: 2
        stride: 2
    }
}

layer {
    name: "lm_conv3"
    type: "Convolution"
    bottom: "lm_pool2"
    top: "lm_conv3"
    convolution_param {
        num_output: 60
        kernel_size: 3
    }
}

#layer {
#  bottom: "lm_conv3"
#  top: "lm_conv3"
#  name: "bn_lm_conv3"
#  type: "BatchNorm"
  #batch_norm_param {
  #  use_global_stats: false
  #}
#}

#layer {
#  bottom: "lm_conv3"
#  top: "lm_conv3"
#  name: "scale_lm_conv3"
#  type: "Scale"
#  scale_param {
#    bias_term: true
#  }
#}

layer {
    name: "lm_relu3"
    type: "ReLU"
    bottom: "lm_conv3"
    top: "lm_conv3"
}

layer {
    name: "lm_pool3"
    type: "Pooling"
    bottom: "lm_conv3"
    top: "lm_pool3"
    pooling_param {
        pool: MAX
        kernel_size: 2
        stride: 2
    }
}

layer {
    name: "lm_conv4"
    type: "Convolution"
    bottom: "lm_pool3"
    top: "lm_conv4"
    convolution_param {
        num_output: 80
        kernel_size: 2
    }
}

#layer {
#  bottom: "lm_conv4"
#  top: "lm_conv4"
#  name: "bn_lm_conv4"
#  type: "BatchNorm"
  #batch_norm_param {
  #  use_global_stats: false
  #}
#}

#layer {
#  bottom: "lm_conv4"
#  top: "lm_conv4"
#  name: "scale_lm_conv4"
#  type: "Scale"
#  scale_param {
#    bias_term: true
#  }
#}

layer {
    name: "lm_relu4"
    type: "ReLU"
    bottom: "lm_conv4"
    top: "lm_conv4"
}

layer {
    name: "lm_fc1"
    type: "InnerProduct"
    bottom: "lm_conv4"
    top: "lm_fc1"
    inner_product_param {
        num_output: 120
    }
}

layer {
    name: "lm_relu_fc1"
    type: "ReLU"
    bottom: "lm_fc1"
    top: "lm_fc1"
}

layer {
    name: "lm_fc2"
    type: "InnerProduct"
    bottom: "lm_fc1"
    top: "lm_fc2"
    inner_product_param {
        num_output: 10
    }
}
